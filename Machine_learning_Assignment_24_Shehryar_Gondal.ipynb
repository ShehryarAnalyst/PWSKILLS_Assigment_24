{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c66fef4c",
   "metadata": {},
   "source": [
    "## Assignment Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1cd14",
   "metadata": {},
   "source": [
    "__Q1. What is Gradient Boosting Regression?__\n",
    "\n",
    "__Ans)__ Gradient Boosting Regression is a machine learning technique that uses an ensemble of weak regression models to create a strong regression model. It is a variant of the Gradient Boosting algorithm, which is based on the principle of iteratively combining weak learners to create a powerful predictive model.\n",
    "\n",
    "In Gradient Boosting Regression, the weak learners are typically decision trees. The algorithm starts by fitting an initial decision tree to the training data and making predictions. Then, it calculates the residuals (the difference between the actual target values and the predicted values) and fits a new decision tree to these residuals. The process is repeated iteratively, with each new decision tree being trained on the negative gradients (partial derivatives) of the loss function with respect to the current predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612ea69",
   "metadata": {},
   "source": [
    "__Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.__\n",
    "\n",
    "__Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Define the Gradient Boosting Regressor class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators  # Number of trees\n",
    "        self.learning_rate = learning_rate  # Learning rate or shrinkage parameter\n",
    "        self.max_depth = max_depth  # Maximum depth of each tree\n",
    "        self.trees = []  # List to store the individual decision trees\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the residuals with the target values\n",
    "        residuals = y.copy()\n",
    "\n",
    "        # Build each tree in the ensemble\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Train a decision tree on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the residuals by subtracting the predictions of the new tree\n",
    "            residuals -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Store the tree in the ensemble\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the target values by summing the predictions of all trees\n",
    "        y_pred = np.sum([tree.predict(X) for tree in self.trees], axis=0)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Make predictions using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "mse, r2 = evaluate(y_test, y_pred)\n",
    "print(\"Best Model - Mean Squared Error (MSE):\", mse)\n",
    "print(\"Best Model - R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bc852",
   "metadata": {},
   "source": [
    "__Q4. What is a weak learner in Gradient Boosting?__\n",
    "\n",
    "A weak learner in Gradient Boosting refers to a base or individual model that performs only slightly better than random guessing on a given learning task. In the context of Gradient Boosting, weak learners are typically decision trees with a shallow depth or limited complexity. These weak learners are often referred to as \"decision stumps\" when they consist of decision trees with only a single split. The key characteristic of a weak learner is that it produces predictions that are slightly better than random chance, making it useful for building an ensemble model through boosting.\n",
    "\n",
    "__Q5. What is the intuition behind the Gradient Boosting algorithm?__\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively build an ensemble model by combining the strengths of multiple weak learners to create a stronger and more accurate predictive model. The algorithm aims to improve upon the shortcomings of each weak learner by focusing on the mistakes made by the previous models in the ensemble. It does this by assigning higher weights to the misclassified samples and adjusting the subsequent weak learners to prioritize those samples during training. By repeatedly adding weak learners to the ensemble and updating their weights, the algorithm gradually reduces the overall error and produces a strong learner with improved predictive performance.\n",
    "\n",
    "__Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?__\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. It starts with an initial weak learner, which is often a simple model that provides rough predictions for the target variable. Then, for each subsequent iteration, it fits a new weak learner to the residuals or errors of the previous predictions. The new weak learner is trained to minimize the residual errors by adjusting its parameters. The predictions of all the weak learners are then combined to produce the final ensemble prediction. Importantly, the algorithm assigns weights to each weak learner to determine their individual contributions to the ensemble, with higher weights given to more accurate models.\n",
    "\n",
    "__Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?__\n",
    "\n",
    "The mathematical intuition of the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "- Initialize the ensemble model by assigning initial predictions to all samples in the dataset.\n",
    "\n",
    "- Calculate the residuals or errors between the actual target values and the predictions made by the ensemble model.\n",
    "\n",
    "- Train a weak learner, such as a decision tree, on the residuals to fit the errors.\n",
    "\n",
    "- Update the ensemble model by adding the predictions of the newly trained weak learner, multiplied by a learning rate, to the previous predictions.\n",
    "\n",
    "- Repeat steps 2 to 4 for a predetermined number of iterations or until a certain stopping criterion is met.\n",
    "\n",
    "- Combine the predictions of all weak learners in the ensemble, typically through a weighted sum, to obtain the final prediction of the Gradient Boosting model.\n",
    "\n",
    "- Optionally, apply regularization techniques, such as shrinkage or subsampling, to improve the generalization and prevent overfitting.\n",
    "\n",
    "__By iteratively adjusting the weak learners based on the errors of the ensemble model, the Gradient Boosting algorithm progressively improves the model's predictive accuracy and builds a strong learner capable of capturing complex patterns in the data.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
